%SEC%
\section{General Definitions and Theorems}
\label{SEC_"General Definitions and Theorems"}
This section will discuss those definitions and theorems that will be applicable and refferenced later in the document.

%SUB%
\subsection{Methods}

In this document we will look at various functions, such as root functions, trigonometric functions, among others. Despite the variety of functions being analysed there are several methods that are useful for more than one function, we will discuss these methods below.

%SUBSUB%
\subsubsection{Newton-Raphson Method}
\label{SUBSUB_"Newton-Raphson Method"}
\theoremstyle{definition}
\newtheorem{Newton Method}{Definition}[subsubsection]

The Newton-Raphson Method is named after Sir Isaac Newton and Joseph Raphson. It is a method that takes a continuously differentiable function \(f\) and it's derivative \(f'\), as well as an initial guess \(x_0\), to create successively more accurate solutions to \(x\) where \(f(x) = 0\).\\

The specific definition of the Newton-Raphson method that I will be using in this document is below:

%DEF%
\begin{Newton Method}
\label{DEF_"Newton-Raphson Method"}
Given \(f \in \mathcal{C}(\R)\), \(f'\) being the derivative of \(f\), and \(x_0 \in \R\); then we define:
\begin{displaymath}
	x_{n+1} := x_n - \frac{f(x)}{f'(x)} \forall n \in \N
\end{displaymath}
\end{Newton Method}

The hope is that, if we start with a good initial guess, that the method will converge to some \(x \in \R : f(x) = 0\).\\

We derive the above method in the following way. If we have an approximation \(x_n \in \R : f(x_n) \approx 0\) we consider the tangent to \(f(x)\) at \(x_n\), which is given by \(y = f'(x_n)(x-x_n) + f(x_n)\). If we set \(y = 0\) and solve for \(x\) we get that \[x = x_n - \frac{f(x)}{f'(x)}\]

The iterative formula follows by letting \(x_{n+1} := x\) in the above.\\

\TODO{Re-arrange into a better order}

\TODO{Refferences}

\TODO{Possibly include discussions of pros and cons}

%SUBSUB%
\subsubsection{Taylor Series Expansion}
\label{SUBSUB_"Taylor Series"}
\theoremstyle{definition}
\newtheorem{Taylor Series}{Definition}[subsubsection]
\newtheorem{Taylor Polynomial}[Taylor Series]{Definition}

The Taylor Series formulation was created by Brook Taylor in 1715, based off of the work of Scottish mathematician James Gregory. The Taylor Series describes a method of representing a given function by a polynomial, where any finite number of terms will give an approximation to the function itself.

%DEF%
\begin{Taylor Series}
\label{DEF_"Taylor Series"}
Given \(f : \R \to \R\) which is infinitely differentiable at \(a \in \R\), we define the Taylor Series of \(f\) at \(a\) to be:
\[\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n\]
\end{Taylor Series}

We can then, from our definition of a Taylor Series, define a polynomial that will approximate our function \(f\) at \(x \in \mathcal{I} \subset \R\)

%DEF%
\begin{Taylor Polynomial}
\label{DEF_"Taylor Polynomial"}
Given \(f : \R \to \R\) which has a Taylor Series of
\( \sum_{n=0}^\infty c_n x^n \), we define the Taylor Polynomial of degree \(N \in \N\) to be
\[ p_N(x) := \sum_{n=0}^N c_n x^n = c_0 + c_1 x + c_2 x^2 + \dotsb + c_N x^N\]
\end{Taylor Polynomial}

Some examples of simple Taylor Series are:
\begin{align*}
\frac{1}{1-x} &= \sum_{n=0}^\infty x^n &\forall x \in {({-1},1)}\\
\end{align*}

\TODO{Eloquate this section}

\TODO{Discuss when a Taylor Series is not equal to it's function}

\TODO{Describe intervals for which Taylor Series converge}

\TODO{Create more examples of Taylor Series that are not used later in the document}

\TODO{Possibly prove Taylor's Theorem}

%SUBSUB%
\subsubsection{CORDIC}
\label{SUBSUB_"CORDIC"}
\TODO{Describe the CORDIC Algorithm in general}

\TODO{Let other sections define specific changes to CORDIC}

%SUB%
\subsection{Errors}
\label{SUB_"Error Definitions"}
\theoremstyle{definition}
\newtheorem{Absolute Error}{Definition}[subsection]
\newtheorem{Relative Error}[Absolute Error]{Definition}
\newtheorem{Iteration Error}[Absolute Error]{Definition}

Errors are very useful in this document for discussing convergence and measuring the accuracy of a particular method. There are two distinct types of error that we are interested in:

%DEF%
\begin{Absolute Error}
\label{DEF_"Absolute Error"}
If we have a value \(v\) and it's approximation \(\tilde{v}\), then the absolute error is
\[ \epsilon := \left| v - \tilde{v} \right| \]
\end{Absolute Error}

The absolute error will be useful in guaranteeing a certain level of accuracy that a given implementation of a method will give. Uses of absolute error in the document will use \(\epsilon\) as their absolute error variable.

%DEF%
\begin{Relative Error}
\label{DEF_"Relative Error"}
If we have a value \(v\) and it's approximation \(\tilde{v}\), then the relative error is
\[ \eta := \frac{\epsilon}{\left| v \right|} = \left| 1 - \frac{\tilde{v}}{v} \right|\]
\end{Relative Error}

The relative error will also be useful in guaranteeing a certain level of accuracy that a given implementation of a method will give, particularly when the absolute error varies with the size of the input. Uses of relative error in the document will use \(\eta\) as their absolute error variable.\\

As the previous two errors are hard or impossible to accurately estimate, in a practical manner, then we want an error estimate that we can calculate in an algorithm. Thus we define the iteration error, as the absolute difference of consecutive iterations.

%DEF%
\begin{Iteration Error}
\label{DEF_"Iteration Error"}
If we have the sequence \(\left(x_n\right)_{n \in \N}\), then the iteration error is defined as:
\[ \delta_n := \left|x_n - x_{n-1}\right| \]
\end{Iteration Error}

For our practical purposes we can note that it is almost impossible to calculate \(\epsilon_n\) exactly, while \(\delta_n\) is easy to calculate and tends to be a good enough approximation of \(\epsilon_n\); particularly if the convergence is rapid.\\ 

%SUB%
\subsection{Convergence}
\label{SEC_"Convergence"}

\thoremstyle{definition}
\newtheorem{Uniform Convergence}{Definition}[subsection]
\newtheorem{Rate of Convergence}[Uniform Convergence]{Definition}

\theoremstyle{remark}
\newtheorem{Uniform Convergence R1}{Remark}[Uniform Convergence]

\theoremstyle{plain}
\newtheorem{Uniform Convergence Thm}{Theorem}[subsection]
\newtheorem{Quad Convergence of Newton}[Uniform Convergence Thm]{Theorem}

In this section we consider what it means for a sequence to converge to a limit value, and some results that will be useful in future chapters.

%DEF%
\begin{Uniform Convergence}
\label{DEF_"Uniform Convergence"}
A sequence \((x_n \in \R: n \in \N)\) converges to \(x\) uniformly if \[\forall \tau \in \Rpz \exists N \in \N : \epsilon_n := |x - x_n| < \tau \forall n \in [N, \infty) \cap \Z\]
\end{Uniform Convergence}

%RMK%
\begin{Uniform Convergence R1}
\label{RMK_"Uniform Convergence R1"}
We will typically use the notation that \(\lim_{n \to \infty} |x_n - x| = 0\), to denote that \((x_n : n \in \N)\) converges to \(x\).
\end{Uniform Convergence R1}

%THM%
\begin{Uniform Convergence Thm}
\label{THM_"Uniform Convergence Thm"}
\((x_n \in \R : n \in \N)\) converges to \(x\) uniformly if and only if \[\forall \tau \in \Rpz \exists N \in \N : |x_n - x_m| < \tau \forall m, n \in [N, \infty) \cap \Z\]
\end{Uniform Convergence Delta}
\begin{proof}
For \(\implies\):\\
Suppose that \((x_n : n \in \N)\) converges to \(x\) uniformly. Then \(\forall \tau \in \Rpz \exists N \in \N : |x_n - x| < \tau \forall n \in [N, \infty) \cap \Z\).\\
Thus suppose \(N \in \N\) is such that \(|x_n - x| < \frac{\tau}{2} \forall n \in [N, \infty) \cap \Z\).\\
Then if \(n, m \ge N\) we see that \[|x_n - x_m| \le |x_n - x| + |x_m - x| \le \tau\]\\

For \(\Leftarrow\):\\
Omitted for brevity.
\end{proof}

\begin{Rate of Convergence}
If \((x_n \in \R : n \in \N)\) is a sequence that converges to \(x\), then it is said to converge:
\begin{itemize}
\item Linearly if \(\lambda \in \Rp\) and \[\lim_{n\to\infty}\frac{|x_{n+1} - x|}{|x_n - x|} = \lambda\]
\item Quadratically if \(\lambda \in \Rp\) and \[\lim_{n\to\infty}\frac{|x_{n+1} - x|}{|x_n - x|^2} = \lambda\]
\item Order \(\alpha \in \Rpz\) if \(\lambda \in \Rp\) and \[\lim_{n\to\infty}\frac{|x_{n+1} - x|}{|x_n - x|^\alpha} = \lambda\]
\end{itemize}
\end{Rate of Convergence}

For the following proof we will have \(\epsilon_n := |x^\ast - x_n|\).

%THM%
\begin{Quad Convergence of Newton}
\label{THM_"Quad Conv Newton"}
Let \(f\) be a twice differentiable function, \(x^\ast\) be a solution to \(f(x) = 0\) and \((x_n : n \in \N)\) be a sequence produced by the Newton-Raphson Method from some initial point \(x_0\). If the following are satisfied, then \((x_n : n \in \N_0)\) converges quadratically to \(x^\ast\):
\begin{description}

\item[\(\textrm{NR}_1\):]
\begin{math}
	f'(x) \neq 0 \forall x \in I := [x^\ast - r, x^\ast + r], \ \mathrm{where}\ r \in \left[\left|x^\ast - x_0\right|, \infty\right)
\end{math}

\item[\(\textrm{NR}_2\):]
\begin{math}
	f''(x) \ \textrm{is continuous}\  \forall x \in I
\end{math}

\item[\(\textrm{NR}_3\):]
\begin{math}
	M\left|\epsilon_0\right| < 1 \ \mathrm{where}\ M := \sup\left\{\left|\frac{f''(x)}{f'(x)}\right| : x \in I\right\}\\
\end{math}
\end{description}
\end{Quad Convergence of Newton}

\begin{proof}
By Taylor's Theorem with Lagrange Remainders we have that
\begin{displaymath}
	0 = f(x^\ast) = f(x_n) + (x^\ast - x_n)f'(x_n) + \frac{1}{2}
		(x^\ast - x_n)^2f''(y_n)
\end{displaymath}
where \(0 < |x^\ast - y_n| < |x^\ast - x_n|\).\\

Then we get the following derivation:
\begin{displaymath}
\begin{align*}
	&f(x_n) + (x^\ast - x_n)f'(x_n) = 
		-\frac{1}{2}(x^\ast - x_n)^2f''(y_n)\\
	\implies &(\frac{f(x_n)}{f'(x_n)} - x_n) + x^\ast =
		-\frac{1}{2}\frac{f''(y_n)}{f'(x_n)}(x^\ast - x_n)^2
		&\textrm{as} \ \textrm{NR}_3 \implies f'(x_n) \neq 0\\
	\implies &x^\ast - x_{n+1} = 
		-\frac{1}{2}\frac{f''(y_n)}{f'(x_n)}(x^\ast - x_n)^2\\
	\implies &\epsilon_{n+1} =
		\frac{1}{2}\left|\frac{f''(y_n)}{f'(x_n)}\right|\epsilon_n^2
		&\textrm{by taking absolute values}
\end{align*}
\end{displaymath}
As \(\textrm{NR}_2\) holds then \(M\) exists and is positive, and thereforewe have:
\[\epsilon_n \le M\epsilon_{n-1}^2 \le M^{2^n - 1}\epsilon_0^{2^n}\]

We now aim to show that we have convergence, i.e. \(\lim_{n \to \infty} x_n = x^\ast\); to do this it suffices to show that \(\lim_{n\to\infty}\epsilon_n = 0\).\\

Consider the sequence \((z_n := M^{2^n - 1}\epsilon_0^{2^n} : n \in \N_0)\). We know that \(0 \le \epsilon_n \le z_n \forall n \in \N_0\), so it then follows that if \(\lim_{n \to \infty}z_n = 0\), then \(\lim_{n \to \infty}\epsilon_n = 0\) by the Squeeze Theorem \ref{#BIBREF#}.\\

Now as \(M\epsilon_0 < 1\) by \(\textrm{NR}_3\), then we see that:

\begin{displaymath}
\begin{align*}
\lim_{n\to\infty}z_n 
	&= \lim_{n\to\infty}(M\epsilon_0)^{2^n - 1}\epsilon_0\\ 
	&= \epsilon_0\lim_{n\to\infty}(M\epsilon_0)^{2^n - 1}\\
	&= \epsilon_0\cdot0
		&\textrm{because } \lim_{n\to\infty}r^{m_n} = 0 \textrm{where }
			|r| < 1, m_n \ge n \forall n \in \N\\
	&= 0
\end{align*}
\end{displaymath}

Now to show that this sequence converges quadratically we see that \(\epsilon_{n+1} = \frac{1}{2}\left|\frac{f''(y_n)}{f'(x_n)}\right|\epsilon_n^2\), and therefore \(\frac{\epsilon_{n+1}}{\epsilon_n^2} = \frac{1}{2}\left|\frac{f''(y_n)}{f'(x_n)}\right|\).\\

Because \(|x^\ast - y_n| < |x^\ast - x_n|\) and \(\lim_{n\to\infty}x_n = x^\ast\), then it follows that \(\lim_{n\to\infty}y_n = x^\ast\). Therefore we see that
\[\lim_{n\to\infty}\frac{\epsilon_{n+1}}{\epsilon_n} = \frac{1}{2}\left|\frac{f''(x^\ast)}{f'(x^\ast)}\right| \in \Rp\]

Hence as the above limit exists and is positive then the sequence is quadratically convergent.
\end{proof}

%SUB%
\subsection{Efficiency and Accuracy Metrics}
\label{SUB_"Efficiency and Accuracy Metrics"}

