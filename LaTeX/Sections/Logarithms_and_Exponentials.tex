\section{Logarithms and Exponentials}

Exponentiation is the operation of calculating \(x^y\) where \(x\) and \(y\) are members of some field, for the purposes of this document we will be considering \(x, y \in \R\). This operation is widely used by many different branches of mathematics and industry, for example many real world phenomena can be modelled by exponentials; we would therefore like to be able to calculate \(x^y\) quickly and efficiently.\\

The first thing we consider is that \(x^y\) when \(x \in \Rn\) and \(y \in \R\setminus\Z\) is not well-defined on \(\R\), and requires consideration of the function on the complex plane. Due to this we will not be considering negative numbers to non-integer bases; in particular, unless stated otherwise, we will be assuming that \(x \in \Rpz\).\\

Now we also know that \(x^{-y} = \frac{1}{x^y}\) when \(y \in \R\), and as such we will also be restricting this section to the assumption that \(y \in \Rpz\). Further we consider the following facts: 

\[x^0 = 1 \forall x \in \Rpz\]
\[0^y = 0 \forall y \in \Rp\]

If we take out these known trivial cases then we can restrict this section to considering only \((x, y) \in \Rp^2\).\\

Now if we have \(y \in \Rp\) then it follows that \(\exists (a, b) \in \Zpz \times [0,1)\) such that \(y = a + b\). This allows us to use the identy that \(x^{m+n} = x^mx^n\) to consider the following two cases seperately:

%EQN%
\begin{equation}
\label{EQN_"exp case 1"}
	x^a : a \in \Zpz
\end{equation}
\begin{equation}
\label{EQN_"exp case 2"}
	x^b : b \in [0,1)
\end{equation}

%SUB%
\subsection{Calculating \(x^a\)}

As we know that \(a \in \Zpz\), then we know that \(x^a = \underbrace{x\times \cdots \times x}_a\); i.e. the problem is equivalent to finding \(x\) multiplied with itself \(a\) times. As we are only dealing with \(a \in \Zpz\), then we will be considering \(x \in \R\) as we can calculate exponentials of negative numbers.\\

The naive way to go about calculating \(x^a\) is to simply perform the multiplication of \(x\) by itself \(a\) times. The algorithm for that can be seen below:

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Naive integer exponentiation},label={PCD_"Naive int exp"}]
  naive_int_exp($x \in \R, a \in \Zpz$):
      $n := 0$
      $z := 1$
      while $n < a$:
          $z \mapsto x\cdot z$
      return $z$
\end{lstlisting}

This algorithm is very simple and has complexity of \(\bigO(a)\), which is a reasonable complexity, but still has the chance to grow large as \(a\) grows. Instead we can consider a more informed approach, in particular we know that either \(2 \mid a\) or \(2 \nmid a\), which then gives us the following:

\begin{displaymath}
	x^a = \left\{\begin{array}{lcl}
		(x^2)^{\tfrac{a}{2}} & : & 2 \mid a\\
		x \cdot (x^2)^{\tfrac{a-1}{2}} & : & 2 \nmid a
	\end{array}\right.
\end{displaymath}

We can use this fact to build a recursive method of calculating \(x^a\), where we repeatedly call the method from within itself. To ensure the method ends correctly we need to identify a base case for the recursion, i.e. where the process stops and returns the correct value. We can see that eventually the above will reach the point where \(a = 0\), in which case we know that \(x^0 = 1\); this will be the base case of our recursion.\\

We want to ensure that the algorithm will terminate, which we can do by seeing that it terminates when \(a = 0\) and then considering \(a \in \Zp\). Now if \(2 \mid a\) then \(\tfrac{a}{2} \in \Zp\) and also \(\tfrac{a}{2} < a\), similarly if \(2 \nmid a\) then \(\tfrac{a-1}{2} \in \Zpz\) because \(a \ge 1\) and also \(\tfrac{a-1}{2} < a\). Thus we see that the sequence produced by \(a \in \Zp\) is a strictly decreasing sequence that is bounded below by 0 and thus we must eventually reach 0, meaning the algorithm terminates.\\

Instead of a recursive algorithm that calls itself the algorithm below is an iterative version which performs the same function:

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Exponentiation by squaring},label={PCD_"exp by square"}]
  exp_by_squaring($x \in \R, a \in \Zpz$):
      $n := a$
      $z := 1$
      $\hat{x} := x$
      while $n > 0$:
          if $2 \nmid n$:
              $z \mapsto \hat{x} \times z$
              $n \mapsto n - 1$
          $\hat{x} \mapsto \hat{x}^2$
          $n \mapsto \tfrac{n}{2}$
      return $z$
\end{lstlisting}

This algorithm is much more efficient than Algorithm \ref{PCD_"Naive int exp"} due to the number of times the inner loop is executed. The inner loop drives \(a\) towards 0 by dividing by 2 each step, this means that as \(a = \bigO(2^{\log_2(a)})\), then this goal is acheived in only \(\log_2(a)\) loops. Therefore the complexity of this algorithm is \(\bigO(\log_2(a))\), which is an improvement upon the previous algorithm's complexity of \(\bigO(a)\).\\

To see this difference in efficiency in action the following table shows the times taken for each method when comparing 1000 different pairs of values \((x, a) \in [0, 10]\times([0,100]\cap\Z)\). With these values we calculated \(x^a\) using both methods 100000 times to get the following results:

%TBL%
{\fontfamily{pcr}\selectfont
\begin{center}
\begin{tabular}{|c|r|r|}
	\hline
	& \codeinline{naive\_int\_exp} & \codeinline{squaring\_int\_exp}
		\\\hline
	\textsf{Total time:} & 16.800s & 2.593s\\\hline
	\textsf{Average time:} & 0.016s & 0.002s\\\hline
	\textsf{Minimum time:} & 0.000s & 0.000s\\\hline
	\textsf{Maximum time:} & 0.037s & 0.004s\\\hline
\end{tabular}
\end{center}}

With this we will move on to further subsections as there are few improvements that can be made on an \(\bigO(\log_2(a))\) algorithm, particularly in this instance.

%SUB%
\subsection{Calculating \(x^b\)}

If we have \(b \in (0, 1)\), then we obviously can't use the our previous sbusection for calculating \(x^y\). The most common way of calculating such exponentiation is by considering that \(x = e^{\ln(x)}\) and thus \(x^b = (e^{\ln(x)})^b = e^{b\ln(x)}\); however this now raises the problem of how to calculate both \(e^\alpha\) and \(\ln(\beta)\). The following will deal with how to calculate these values and thus use them in conjunction to calculate \(x^b\).\\

%SUB%
\subsection{TBC}

The mathematical constant \(e\) has been known since the early 1600s and was originally calculated by Jacob Bernoulli, and was studied bye Leonhard Euler, where it appeared in Euler's Mechanica in 1736. While several possible equivalent definitions of \(e\) exist the most common such definition is that \(e := \lim_{n \to \infty}(1 + \tfrac{1}{n})^n\).\\

If we now consider the definition of \(e\) and also consider \(e^x\), then we can show that \(e^\alpha = \lim_{n\to\infty}(1+\tfrac{x}{n})^n\). This gives us our first basic method of how to calculate \(e^x\):

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Baisc Method for calculating \(e^\alpha\)},label={PCD_"basic exp"}]
  basic_exp($x \in \R, n \in \N$)
      return $(1 + \tfrac{x}{n})^n$
\end{lstlisting}

If we consider \((1+\tfrac{x}{n})^n\) as a function of a continuous \(n\) then we can find the following derivation:

\begin{align*}
	\frac{d}{dn}\left[(1 + \frac{x}{n})^n\right]
		&= (1+\frac{x}{n})^n\frac{d}{dn}\left[n\ln(1+\frac{x}{n})\right]\\
	&= (1+\frac{x}{n})^n(\frac{d}{dn}[n]\ln(1+\frac{x}{n}) 
		+ n\frac{d}{dn}\left[\ln(1+\frac{x}{n})\right])\\
	&=(1+\frac{x}{n})^n(\ln(1+\frac{x}{n}) 
		+ \frac{n}{1 + \frac{x}{n}}\frac{d}{dn}(1 + \frac{x}{n}))\\
	&=(1+\frac{x}{n})^n(\ln(1+\frac{x}{n}) - \frac{x}{n + x})\\
	&=\frac{(1+\frac{x}{n})^n}{x + n}((x + n)\ln(1 + \frac{x}{n}) - x)
\end{align*}

By the last line of this we can see that because \((x, n) \in \Rp^2\) then \(\ln(1 + \frac{x}{n} > 0\) and thus we conclude that \((x+n)\ln(1+\frac{x}{n}) - x > 0\). Therefore we see that \(\frac{d}{dn}\left[(1+\frac{x}{n})^n\right] > 0\) for all \((x, n) \in \Rp^2\), and in particular this means that \((1+\frac{x}{n})^n < (1+\frac{x}{n+1})^{n+1} \forall n \in \N\).\\

One consequence of this is that \((1+\frac{x}{n})^n < e^x \forall n \in \N\), therefore we can define the error of algorithm \ref{PCD_"basic exp"} as \(\epsilon_N := |e^x - (1+\frac{x}{n})^n| = e^x - (1+\frac{x}{n})^n\). Now as \(\lim_{n\to\infty}(1+\frac{x}{n})^n = e^x\) then we see that \(\lim_{n\to\infty}\epsilon_n = 0\), and thus our algorithm is correct and valid for approximating \(e^x\).\\

Next we see that this method, while simple, approximates \(e^x\) very poorly. In particular the table below shows the approximation of \(e^{0.75}\) for differnt values of \(n\), where the bold digits are the correctly approximatd digits.

%TBL%
{\fontfamily{pcr}\selectfont
\begin{center}
\begin{tabular}{|l|l|}
\hline
\(n\) & \textsf{Approximation of \(e^{0.75}\)}\\\hline
1 & 1.800000000000000044\\\hline
10 & \textbf{2.}158924997272786787\\\hline
100 & \textbf{2.2}18468215957572747\\\hline
1000 & \textbf{2.22}4829248807374831\\\hline
10000 & \textbf{2.225}469716120127850\\\hline
100000 & \textbf{2.2255}33806810873500\\\hline
1000000 & \textbf{2.225540}216319864358\\\hline
10000000 & \textbf{2.225540}857275162929\\\hline
100000000 & \textbf{2.22554092}1370736781\\\hline
1000000000 & \textbf{2.22554092}7780294606\\\hline
\end{tabular}
\end{center}}

With this table we see that the method very poorly approximates \(e^x\), requiring a very large \(n\) to get just a few digits of accuracy. While this does not require more calculations from the method, requiring this large a value of \(n\) can lead to inaccuracies in the implementation of the algorithm using \codeinline{double} data types in C.\\

In general there are better methods of approximating \(e^x\) and also \(\ln(x)\), which while requiring more calculations are much more accurate than the most basic method presented here.

%SUB%
\subsection{Taylor Series Method}

\theoremstyle{plain}
\newtheorem{nat log dif}{Proposition}[subsection]
\newtheorem{log convergence}[nat log dif]{Proposition}

If we take the elementary result from calculas that \(\frac{d}{dx}e^x = e^x\), then we can calculate the McClaurin series of \(e^x\). By the definition of a McClaurin series we know that the series expansion of \(e^x\) about 0 is 

\[\sum_{k=0}^\infty \frac{d^{(k)}}{dx^k}[e^x](0)\frac{x^k}{k!}\]

As \(\frac{d^{(k)}}{dx^k}[e^x] = e^x \forall k \in \Zpz\) and \(e^0 = 1\) then we see that the series becomes

\[\sum_{k=0}^\infty \frac{x^k}{k!}\]

Using this we see that \(e^x \approx \sum_{k=0}^n \frac{x^k}{k!}\), which gives the following method for approximating \(e^x\):

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Taylor Method for calculating \(e^x\)},label={PCD_"taylor exp"}]
  taylor_exp($x \in \R, n \in \Zpz$)
      $t = 1$
      $z = 1$
      $k = 1$
      while $k < n$:
          $t \mapsto \frac{t\cdot x}{n}$
          $z \mapsto z + t$
          $k \mapsto k + 1$
      return $z$
\end{lstlisting}

This allows us to calculate \(e^x\) more efficiently as we can note that the error of the approximation is easy to approximate. We know that \(\epsilon_n := |e^x - \sum_{k=0}^n\frac{x^k}{k!}| \le \frac{|x|^{n+1}}{(n+1)!}\) for all \(n \in \Zpz\). While we can't guarantee the size of \(x\) in general we will consider \(x \in (0,1)\) for the purposes of analysing this function.\\

As \(x \in (0,1)\) then it follows that \(x < 1\) and thus we can see that \(\epsilon_n < \frac{1}{n!} \forall n \in \Zpz\). Using this we can see that if we wish to use our method such that the error is at most \(\tau_d := 10^{-d}\), then we need to find \(\n \in \Zpz : \frac{1}{n!} < \tau_d\). The table below shows some values for \((n, d)\) pairs such that \(n\) is the smallest positive integer such that \(\frac{1}{n!} < \tau_d\):

%TBL%
\begin{center}
\begin{tabular}{|l|l|}
\hline
\(d \in \N\) 
	& \(\textrm{arg}\max\left\{n \in \N : n! > 10^d\right\}\)\\\hline
1 & 4\\\hline
10 & 14 \\\hline
100 & 70 \\\hline
1000 & 450\\\hline
\end{tabular}
\end{center}

Thus we see that that we can guarantee 100 digits of accuracy with an input of \(n \ge 70\) and 1000 digits of accuracy with \(n \ge 450\), this is much less than our previous method where an input of \(n = 1000\) only gave 2 decimal places of accuracy.\\

The inverse of the function \(z = e^x\) is the logarithm function \(\ln(z) = x\), which we can again consider for Taylor Series expansion. First we will show the result from calculas that \(\frac{d}{dx}[\ln(x)] = \frac{1}{x}\):

%THM%
\begin{nat log dif}
\[\frac{d}{dx}[\ln(x)] = \frac{1}{x}\]
\end{nat log dif}
\begin{proof}
We will prove this from the first principles using the definition that \(\frac{d}{dx}[f(x)] = \lim_{h\to 0} \frac{f(x + h) - f(x)}{h}\)

\begin{align*}
	\frac{d}{dx}[\ln(x)] 
		&= \lim_{h\to 0}\frac{\ln(x + h) - \ln(x)}{h}\\
		&= \lim_{h\to 0}\frac{\ln(1 + \frac{h}{x})}{h}\\
		&= \lim_{h\to 0}\ln\left((1+\frac{h}{x})^{\frac{1}{h}}\right)
\end{align*}

If we let \(u := \frac{h}{x}\), then we get that \(ux = h\) and \(\frac{1}{h} = \frac{1}{ux}\). Also \(\lim_{h\to 0}u = 0\), and so we get the following:

\begin{align*}
	\frac{d}{dx}[\ln(x)]
		&= \lim_{u\to 0}\ln((1+u)^{\frac{1}{ux}})\\
		&= \frac{1}{x}\lim_{u\to 0}\ln((1+u)^{1/u})\\
\end{align*}

If we now let \(n := \frac{1}{u}\) and consider that \(\lim_{u\to 0} n = \infty\), then our derivative becomes:

\begin{align*}
	\frac{d}{dx}\ln(x) 
		&= \frac{1}{x}\lim_{n\to\infty}\ln((1 + \frac{1}{n})^n)\\
		&= \frac{1}{x}\ln(\lim_{n\to\infty}(1+\frac{1}{n})^n)\\
		&= \frac{1}{x}\ln(e) &\textrm{by the definition of \(e\)}\\
		&= \frac{1}{x}
\end{align*}
\end{proof}

Now we know that \(\frac{d^k}{dx^k}[\frac{1}{x}] = (-1)^kk!x^{-k-1}\), and thus we can build up a Taylor Series expansion. In this case, rather than centering the series about \(x=0\) for a McClaurin series we can instead center the series around \(x=1\) which gives the following series expansion for \(\ln(x)\):

\begin{align*}
	\sum_{k=0}^\infty \frac{\frac{d^k}{dx^k}[\ln(x)](1)}{k!}(x-1)^k
		&=\ln(1) + \sum_{k=1}^\infty\frac{\frac{d^{k-1}}{dx^{k-1}}
			[x^{-1}](1)}{k!}(x-1)^k\\
		&=\sum_{k=1}^\infty\frac{[(-1)^{k-1}(k-1)!x^{-k}](1)}
			{k!}(x-1)^k\\
		&=\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k}(x-1)^k\\
		&=-\sum_{k=1}^\infty\frac{(1-x)^k}{k}
\end{align*}
				
We know that \(\ln(x) = -\sum_{k=1}^\infty\frac{(1-x)^k}{k}\) when the series \(\sum_{k=1}^\infty\frac{(1-x)^k}{k}\) converges. We thus need to know when the sum converges.

%THM%
\begin{log convergence}
The series \(\sum_{k=1}^\infty\frac{(1-x)^k}{k}\) converges when \(x \in (0,2)\).
\end{log convergence}
\begin{proof}
Let \(a_k := \frac{(1-x)^k}{k}\). We will proceed by using the ratio test to show when the series converges absolutely. The test states that the series converges when \(\lim_{k\to\intfy} \left|\frac{a_{k+1}}{a_k}\right| < 1\).\\

Now we can consider the following derivation:

\begin{align*}
	\lim_{k\to\infty}\left|\frac{a_{k+1}}{a_k}\right|
		&=\lim_{k\to\infty}\left|\frac{\frac{1}{k+1}(1-x)^{k+1}}
			{\frac{1}{k}(1-x)^k}\right|\\
		&=\lim_{k\to\infty}\left|\frac{k}{k+1}(1-x)\right|\\
		&=|1-x|\lim_{k\to\infty}\left|\frac{k}{k+1}\right|\\
		&=|1-x|
\end{align*}

Therefore our series converges when:
\begin{align*}
	|1-x| < 1 &\iff -1 < 1 - x < 1\\
		&\iff -1 < x - 1 < 1\\
		&\iff 0 < x < 2
\end{align*}

Hence \(\sum_{k=1}^\infty \frac{(1-x)^k}{k}\) converges when \(x \in (0, 2)\).
\end{proof}

Now as we can't know if \(x \in (0,2)\) then we can consider that \(\forall x \in \Rp \exists (a, b) \in [\frac{1}{2}, 1)\times\Z : x = a\cdot2^b\); thus we see that \(\ln(x) = \ln(a\cdot2^b) = b\ln(2) + \ln(a)\). As previously noted in Section \ref{#REF#} this operation, while theoretically complex, is simple to calculate for most computers by how the represent floating point values.\\

With this we can then use the following method to approximate \(\ln(x)\) by the taylor polynomial \(-\sum_{k=1}^n\frac{(1-x)^k}{k}\):

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Taylor Method for calculating \(\ln(x)\)},label={PCD_"taylor log"}]
  taylor_log($x \in \Rp, n \in \N$):
      Find $(a, b) \in [\tfrac{1}{2}, 1)\times \Z$ such that $x = a\cdot2^b$
      $y := 1 - a$
      $t := y$
      $z := -y$
      $k := 1$
      while $k < n$:
          $t \mapsto t\cdot y$
          $z \mapsto z + \tfrac{t}{k}$
          $k \mapsto k + 1$
      return $z -b\ln(2)$
\end{lstlisting}

\subsection{Hyperbolic Series Method}
\subsection{CORDIC}
\TODO{Fill this out with stuff}
