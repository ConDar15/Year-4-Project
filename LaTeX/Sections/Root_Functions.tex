%SEC%
\section{Root Functions}
\label{SEC_"Root Functions"}

In this section of the document we will consider several methods for approximating root functions. For our purposes we are only going to consider roots of \(N \in \Rpz\), this is because if \(N \in \R^-\) then it follows that \(\sqrt{N} = i\sqrt{|N|}\).

%SUB%
\subsection{Digit by Digit Method}
\label{SUB_"Digit by Digit Method"}

The first method we will examine is an old method, that has been observed in Babylonian Mathematics over 2000 years ago, which is used to accurately generate the square root of numbers one digit at a time. This method differs from others discussed as it generates each digit of the root with perfect accuarcy, one at a time, thus in a theoretical sense this algorithm is the most accurate of the methods we will view; we will see however that this method is slow.\\

Now suppose we are looking for \(\sqrt{N}\), then we know that \(\sqrt{N} = a_010^n + a_110^{n-1} + a_210^{n-2} + \dots\) for some \(n \in \Z\); it then follows that \(N = (a_010^n + a_110^{n-1} + a_210^{n-1} + \dots)^2\). By expanding the quadratic value we get that \[N = a_0^210^{2n} + (20a_0 + a_1)a_110^{2n-2} + (20(a_010 + a_1) + a_2)a_210^{2n-4} + \dots + (20\sum_{i=0}^{k-1}a_i10^{k-i-1} + a_k)a_k10^{2n - 2k}\]

An observation should be made regarding the value of \(n\) that we use for the theorem. We could of course try different values of \(n\), in some structured procedure, that will find the largest \(n\) such that \(10^n \le N\). However we can note that \(log_{10}(\sqrt{N}) = \tfrac{1}{2}log_{10}(N)\), thus \(10^{\frac{1}{2}log_{10}(N)} = \sqrt{N}\). Using this information, and the fact that \(n \in \Z\), we can have \(n := \left\lfloor \tfrac{1}{2}log_{10}(N) \right\rfloor\). \\

This allows us to get successive apporximations of \(N\) where \(N_0 = a_0^210^{2n}\), \(N_1 = N_0 + (20a_0 + a_1)a_110^{2n-2}\), \(N_2 = N_1 + (20(a_010 + a_1) + a_2)a_210^{2n-4}\). This will alllow us to create an algorithm that will give successive approximations of \(sqrt{N} = a_010^n + a_110^{n-1} + \dots\), more importantly each approximation will give us the exact next digit in the decimal representation of \(\sqrt{N}\).\\

Thus we can have an iterative method to solve the problem, where at each stage we are trying to find the largest digit which satisfies the inequality \((20\sum_{i=0}^{k-1}a_i10^{k-i-1} + a_k)a_k10^{2n-2k} \le N - N_{k-1}\). Thus we get the following pseudocode, which outputs two sequences, one indicating the digits before the decimal point and one afterwards. I will use set notation to indicate the sequences, but in this case order is important and repetition is allowed.

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Exact Digit by Digits Square Root}]
  exactRootDigits($N \in \Rpz, d \in \N$):
      $Digits_a := \emptyset$
      $Digits_b := \emptyset$
      $k := 0$
      $n := \left\lfloor\tfrac{1}{2}log_{10}(N)\right\rfloor$
      while $k < d$:
          $a_k := \max\left\{t \in [0, 9] \cap \Z : \left(20\sum_{i=0}^{k-1}a_i10^{k-i-1} + t\right)t10^{2n-2k} \le N\right\}$
          $N \mapsto N - \left(20\sum_{i=0}^{k-1}a_i10^{k-i-1} + a_k\right)a_k10^{2n-2k}$
          if $n-k < 0$:
              $Digits_b \mapsto Digits_b \cup \{a_k\}$
          else:
              $Digits_a \mapsto Digits_a \cup \{a_k\}$
          $k \mapsto k+1$
      if $Digits_a = \emptyset$:
          $Digits_a := \{0\}$
      if $Digits_b = \emptyset$:
          $Digits_b := \{0\}$
      return $(Digits_a, Digits_b)$
\end{lstlisting}

This method has a computational complexity of \(\bigO(d^2)\), as each loop requires the operations of summing \(k\) elements, and the loop is repeated for \(k = 0 \to d\). We will see that by considering some changes to the algorithm we can change the complexity class to be \(\bigO(d)\).\\

First we will note that line 5 is not an issue, as if we only care about the first significant digit of \(\tfrac{1}{2}log_{10}(N)\), then this is \(\bigO(|log(N)|)\). This can be seen as if we start from \(n = 0\) we can either count up or down until a we find \(10^{2n}\) at most or at least N, respectively. This obviously takes at most \(|log_{10}(N)|\) steps, giving us our stated complexity. We will also assume that \(\bigO(|log(N)|) \le \bigO(d)\), as we have already seen that we can manipulate our input N to be within a reasonable range.

Second we note that on line 7 we calculate \(\sum_{i=0}^{k-1}a_i10^{k-i-1}\) for each value of \(t\); we can reduce the complexity of this line by pre-calculating this value. However we can do even better if we consider that at step \(k+1\) we are calculating \(\sum_{i=0}^{k}a_i10^{k-i} = a_k + 10\sum_{i=0}^{k-1}a_i10^{k-i-1}\). Thus if we introduce \(P_0 := 0\), and fore each k we calculate \(P_{k+1} := 10P_k + a_k\), then we can reduce the complexity from \(\bigO(k)\) to \(\bigO(1)\).\\

This calculation of \(P_k\), then carries over to reduce the complexity of line 8 to be \(\bigO(1)\) instead of \(\bigO(k)\). Combining this we can create the modified algorithm below:

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Exact Digit by Digits Square Root version 2}]
  exactRootDigits_v2($N \in \Rpz, d \in \N$):
      $Digits_a := \emptyset$
      $Digits_b := \emptyset$
      $k := 0$
      $n := \left\lfloor\tfrac{1}{2}log_{10}(N)\right\rfloor$
      $P_0 := 0$
      while $k < d$:
          $a_k := \max\left\{t \in [0, 9] \cap \Z : \left(20P_k + t\right)t10^{2n-2k} \le N\right\}$
          $N \mapsto N - \left(20P_k + a_k\right)a_k10^{2n-2k}$
          $P_{k+1} := 10P_k + a_k$
          if $n-k < 0$:
              $Digits_b \mapsto Digits_b \cup \{a_k\}$
          else:
              $Digits_a \mapsto Digits_a \cup \{a_k\}$
          $k \mapsto k+1$
      if $Digits_a = \emptyset$:
          $Digits_a := \{0\}$
      if $Digits_b = \emptyset$:
          $Digits_b := \{0\}$
      return $(Digits_a, Digits_b)$
\end{lstlisting}

This method is usefull, but can be difficult to implement as it requires high precision for the representation of the real value of \(N\). In my implementation using C, I utilised the MPFR library to utilise high precision integers, but still encountered issues regarding loss of precision.\\

As an example the table below shows the number of digits of accuracy I was able to calculate for \(\sqrt{2}\) using the above algorithm, compared to the number of bits of precision used in the calculations.\\

\begin{center}
\begin{tabular}{|p{3cm}|p{3cm}|}
\hline
Bits of Precision & Maximum Accuracy\\ \hline
8 & 2 \\ \hline
16 & 5 \\ \hline
32 & 9 \\ \hline
64 & 18 \\ \hline
128 & 39 \\ \hline
256 & 77 \\ \hline
512 & 154 \\ \hline
1024 & 308 \\ \hline
2048 & 615 \\ \hline
4096 & 1234 \\ \hline
8192 & 2466 \\ \hline
\end{tabular}
\end{center}

This data is highly structured and so we can hope to create a simple function that would allow us to calculate how much precision would be needed for a given number of digits of accuracy, at least for single digit inputs for \(N\). We can see that the average ratio of Precision to Accuracy is 3.41259..., which ranges from 3.31928... to 4.0. From this we can draw a general trend that Digits of Accuracy \(\approx\) 3.4 \(\times\) Bits of Precision; thus if we take the more generous assumption that Digits of Accuracy \(\appprox\) 4 \(\times\) Bits of Precision, we can use this to pre-determine the accuracy needed.\\

It should be noted that to ensure accuracy we should over-estimate the required precision, however if we overestimate the precision, then our calculations will be performed using unnecsarily large data structures and thus computation time will increase.\\

One particular use of this technique is to find an approximation of a squar root to it's integer part, calculated in base 2. This algorithm is of note as we will see that it has a computation time of \(\bigo(1)\).\\

The algorithm uses the same basis as the base 10 version, for it's calculations, but due to the nature of being in binary several changes can be made for computational efficiency. To do this we will view the problem as follows: if we know some \(r \in \Zpz\) which is our current approximation of our root, we are looking for some \(e \in \Zpz\) such that \((r+e)^2 \le N\). Expanding this out we get \(r^2 + 2re + e^2 \le N\), and if we keep track of \(M = N - r^2\), we can test if \(2re + e^2 \le M\).\\

Now we can consider our choice of \(e\), the most practical method is to test successeive \(e_m := 2^m\), where \(m\) is descending starting with \(m = \max{m \in \Zpz : 4^m \le N}\). We can use an iterative formula to build up the integer square root, where we start with \(r = 0, M = N\) and have \(r \mapto r + e_m\) whenever \(2re_m + e_m^2 \le M\), stopping when \(m < 0\). This is then implemented as follows:\\

%PCD%
\begin{lstlisting}[numbers=left,frame=single,mathescape,caption={Integer Square Root Algorithm}]
  integerSquareRoot($N \in \Zpz$):
      $M := N$
      $m := \max{m \in \Zpz : 4^m \le M}$
      $r := 0$
      while $m \ge 0$:
          if $2r(2^m) + 4^m \le M$:
              $M \mapsto M - 2r(2^m) + 4^m$
              $r \mapsto r + 2^m$
          $m \mapsto m - 1$
      return $r$
\end{lstlisting}

If we now conisder an implementation of the above algorithm using an unsigned integer system with \(K\) bits, where \(2 | K\). We will use \codeinline{res} to represent \(2re_m\), which means at the start of the algorihtm we will have \codeinline{res = 0}; similarly we can use \codeinline{bit} to represent \(e_m^2\). As we know that \(K\) bits are used and \(2 | K\), it then follows that the largest power of 4 less than the maximum representable value (\(2^K - 1\) is \(2^{K-2}\), which can be calculated as \codeinline{bit = 1 << (K - 2)} using bitshift operations. Finally we will use \codeinline{num} to represent \(M\).\\

Now that we have discussed the setup we can consider how to implement some of the steps above. First to implement line 3 we can simply keep dividing \codeinline{bit} by 4 while \codeinline{bit > num}, which can be efficiently implemented as \codeinline{bit >> 2} by using bitshifts in place of division by powers of 2. The same technique can be used in place of line 9, which leads us to re-evaluating our usage of line 5. As we are using bitshifting and a bitshift that would take a number past 0 instead results in 0, we also know that \(2 | K\) and so eventually we will reach \codeinline{bit == 1}, which represents \(m = 0\); therefore we can use \codeinline{bit > 0} as our stopping criteria on line 5.\\

Line 6 is easy to convert, given our definitions of \codeinline{res}, \codeinline{bit} and \{num}, as is line 7. All that remains is to consider how to update \codeinline{res}, which has two different ways of being updated depending on whether \codeinline{res + bit <= num}. If it is false that \codeinline{res + bit <= num}, then we wish for \codeinline{res} to represent \(2re_{m-1}\); this is easily acheived if we consider that \(2re_{m-1} = \frac{1}{2}(2re_m)\), which prompts the update \codeinline{res = res >> 1}. For the second case, when \codeinline{res + bit <= num} is true, we want \codeinline{res} to represent \(2(r+e_m)e_{m-1}\); to implement this we consider the following derivation:

\begin{displaymath}
\begin{align*}
	2(r+e_m)e_{m-1} 
		&= \frac{1}{2}\cdot 2(r+e_m)e_m\\
		&= \frac{1}{2}\cdot 2(re_m + e_m^2)\\
		&= \frac{1}{2}(2re_m) + e_m^2
\end{align*}
\end{displaymath}

Using this above derivation we see that we can calculate this as \codeinline{res = (res >> 1) + bit}. Below is a simple implementaion of this in C using the unsigned 32 bit integer type \codeinline{uint32\_t}. A more commented and slightly modified version can be found in Appendix \ref{#APP#}, File \ref{#FILE#}.

\begin{codelisting}{Integer Square Root in C}
uint32_t int_sqrt(uint32_t num)
{
	uint32_t res = 0, bit = (1 << 30);
	
	while (bit > num)
		bit = bit >> 2;
	
	while (bit > 0)
	{
		if (res + bit <= num)
		{
			num = num - (res + bit);
			res = (res >> 1) + bit;
		}
		else
			res = res >> 1;
		
		bit = bit >> 2;
	}

	return res;
}
\end{lstlisting}
\end{codelisting}

We should consider the final step of the loop, when \codeinline{bit == 1}. In this case when \codeinline{res} is updated we have \codeinline{res} represent either \(2(r+e_0)e_{-1} = r + e_0\), or \(2re_{-1} = r\); thus the algorithm exits with the correct value.\\

Now that the algorithm is correctly constructed using simple unsigned integer addition, subtraction and bitshifting (which we can assume all have computational time of \(\bigO{1}\)), we can look at the worst case complexity of the algorithm:

\begin{itemize}
\item The complexity of the set up of variables is constant time.
\item The worst case complexity would be to to have \codeinline{bit <= num} at the start.
\item The loop would execute 16 times for our 32 bit integers, and contains a single operation which is \(\bigO(1)\) complexity.
\begin{itemize}
	\item The worst case within the loop is to have \codeinline{res + bit <= num} for each iteration.
	\item Within the first \codeinline{if} branch there are a constant 4 operations.
	\item Each loop has an additional operation operation to update \codeinline{bit}.
	\item This makes 5 operations per loop, giving \(\bigO(1)\) complexity within the loops.
\end{itemize}
\end{itemize}

Therefore we see that the algorithm has \(\bigO(1)\) time complexity, and even has the same in storage complexity. In particular our 32 bit example requires 163 opertaions, including assignments, comparrisons and calucluations. This means that the integer square root of any number up to 4294967295 can be calculated extremely quickly.

%SUB%
\subsection{Bisection Method}
\label{SUB_"Bisection Method for Roots"}
\theoremstyle{plain}
\newtheorem{Bisection Converges}{Proposition}[subsection]

The Bisection Method is a general method for approximating the zero, \(\alpha\), of a function, \(f\), on a bounded interval, \(I := [a,b]\), where \(f\) has the property \(f(x)f(y) < 0 \forall (x,y) \in [a,\alpha)\times(\alpha, b]\); we may assume, without loss of generality, that \(f(x) < 0 \forall x \in [a, \alpha]\).\\

The bisection method starts with initial bounds \(a_0 = a, b_0 = b\), where the initial approximation for the root is \(x_0 = \frac{1}{2}(a+b)\). We will consider pseudocode of the iteration process, that uses \(b_n - a_a < \tau\) or \(f(x_n) = 0\) as exit criteria. Here \(\tau\) is a tolerance threshold, and if the exit criteria is met it means that \(|x_n - \alpha| \le \frac{\tau}{2}\), while the other exit criteria means we have reached an exact solution.\\

%PCD%
\label{PCD_"General Bisection Method"}
\begin{lstlisting}[frame=single,mathescape,caption={General Bisection Method}]
  bisectionMethod($a \in \R, b \in (a, \infty), f \in \mathcal{C}[a,b], \tau \in \Rp$)
      $a_0 := a$
      $b_0 := b$
      $x_0 := \tfrac{1}{2}(a+b)$
	  $n := 0$
	  while $f(x_n) \neq 0$ AND $b_n - a_n > \tau$:
          if $f(x_n) < 0$:
              $a_{n+1} := x_n$
              $b_{n+1} := b_n$
          else:
              $a_{n+1} := a_n$
              $b_{n+1} := x_n$
          $n \mapsto n+1$
          $x_n := \tfrac{1}{2}(a_n + b_n)$
      return $x_n$
\end{lstlisting}\\
		
For our purposes we are trying to find the zero of \(f(x) = x^2 - N\), which is a strictly increasing function on \(\Rpz\). If \(N >= 1\), then \(\sqrt{N} \in [0, N]\), while \(N < 1 \implies \sqrt{N} \in [0, 1]\). It is obvious that our function has the required property, and thus we get the following method for finding the square root of \(N\):\\

%PCD%
\begin{lstlisting}[frame=single,mathescape,caption={Bisection Method for Square Roots},label={PCD_"Square Root Bisection Method"}]
  bisectionSquareRoot($N \in \Rpz, \tau \in \Rp$)
      $a_0 := 0$
      $b_0 := \max{1, N}$
      $x_0 := \tfrac{1}{2}(a_0 + b_0)$
      $n := 0$
      while $x_n^2 - N \neq 0$ AND $b_n - a_n > \tau$:
          if $x_n^2 - N < 0$:
              $a_{n+1} := x_n$
              $b_{n+1} := b_n$
          else:
              $a_{n+1} := a_n$
              $b_{n+1} := x_n$
          $n \mapsto n+1$
          $x_n := \tfrac{1}{2}(a_n + b_n)$
      return $x_n$
\end{lstlisting}\\

The implementation of this method is efficiently acheived in C using only addition, subtraction and multiplication by a constant. Before this method is implemented, however, we must first consider if and or when it converges to the correct answer. From an intuitive standpoint we would assume that if there is only one root in the interval, it would follow that we would converge to the root.

%THM%
\begin{Bisection Converges}
\label{THM_"Bisecton Converges"}
\(\lim_{n \to \infty} x_n = \sqrt{N}\) for Algorithm \ref{PCD_"Square Root Bisection Method"}
\end{Bisection Converges}
\begin{proof}
To prove this statement it suffices to prove that \(\sqrt{N} \in [a_n, b_n] \forall n \in \N\) and \(\lim_{n\to\infty} |x_n - \sqrt{N}| = 0\).\\

\textit{Claim 1:} \(\sqrt{N} \in [a_n, b_n] \forall n \in \N\)
\begin{subproof}\\
\(a_0 := 0 \implies a_0 \le \sqrt{N}\)\\
\(b_0 := \max\{1, N\} \implies b_0 \ge \sqrt{N}\)\\
Therefore it is obvious that \(\sqrt{N} \in [a_0, b_0]\)\\
Now suppose \(\sqrt{N} \in [a_n, b_n]\) for some \(n \in \N\)\\
It should be noted that \(a_n, b_n, x_n \in \Rpz \forall n \in \N\) as \(a_0, b_0 \in \Rpz\) and all the subsequent values are derived from these using only addition and multiplication by positive factors.\\
We then see that \(x_n := \frac{1}{2}(a_n + b_n)\), and we consider the two cases that \(x_n^2 - N \le 0\) or \(x_n^2 - N \ge 0\).\\
\begin{description}
\item[Case \(x_n^2 - N \le 0\):]\\
	\(a_{n+1} := x_n, b_{n+1} := b_n\)\\
	It is therefore obvious that \(\sqrt{N} \le b_{n+1}\).\\
	Now we see that \(x_n^2 - N \le 0 \implies x_n^2 \le N \implies x_n \le N\) as all the values are non-negative.\\
	Thus \(\sqrt{N} \in [a_{n+1}, b_{n+1}]\).\\
\item[Case \(x_n^2 - N \ge 0\):]\\
	\(a_{n+1} := a_n, b_{n+1} := x_n\)\\
	It is therefore obvious that \(\sqrt{N} \ge a_{n+1}\).\\
	Now we see that \(x_n^2 - N \ge 0 \implies x_n^2 \ge N \implies x_n \ge N\) as all the values are non-negative.\\
	Thus \(\sqrt{N} \in [a_{n+1}, b_{n+1}]\).\\
\end{description}
Hence \(\sqrt{N} \in [a_n, b_n] \implies \sqrt{N} \in [a_{n+1}, b_{n+1}] \forall n \in \N\)\\
As \(sqrt{N} \in [a_0, b_0]\) then we see that \(\sqrt{N} \in [a_n, b_n] \forall n \in \N\)
\end{subproof}\\

\textit{Claim 2:} \(\lim_{n\to\infty}|x_n - \sqrt{N}| = 0\)
\begin{subproof}\\
Let \(n \in \N\) be arbitrary.\\
As \(x_n := \frac{1}{2}(a_n + b_n)\) then we see that \(|a_n - x_n| = |b_n - x_n| = \frac{1}{2}(b_n - a_n)\).\\
Now as \(\sqrt{N} \in [a_n, b_n]\) it follows that \(|\sqrt{N} - x_n| \le \frac{1}{2}(b_n - a_n\).\\
As the modulas function is a mapping from \(\R\) to \(\Rpz\), it is clear that \(|\sqrt{N} - x_n|\) is bounded below by 0.\\
Now as for each \(n \in \N\), either \(a_{n+1} = x_n\) or \(b_{n+1} = x_n\), we see that \(b_{n+1} - a_{n+1} = \frac{1}{2}(b_n - a_n)\). Further we can see that \(b_n - a_n \ge 0 \forall n \in \N\) because \(b_n \ge a_n\).\\
Therefore the sequence of \(\frac{1}{2}(b_n - a_n)\) is a strictly decreasing sequence that is bounded below, by 0. Thus \(\lim_{n\to\infty} \frac{1}{2}(b_n - a_n) = 0\)\\ 
Therefore \(\lim_{n\to\infty} |x_n - \sqrt{N}| = \lim_{n\to\infty}\frac{1}{2}(b_n - a_n) = 0\)
\end{subproof}

By using our two claims above we see that \(\lim_{n\to\infty} x_n = \sqrt{N}\).
\end{proof}

The algorithm can be generalised to search for \(\sqrt{k}{N}\), where \(k \in [2,\infty) \cap \Z\). We can do this by using the integer power function discussed previously in section \ref{SEC#}. This gives the following algorithm:
 
%PCD%
\label{PCD_"Bisection Method for General Roots"}
\begin{lstlisting}[frame=single,mathescape,caption={Bisection Method for General Roots}]
  kRootBisectionMethod($N \in \Rpz, k \in [2, \infty) \cap \Z, \tau \in \Rp$)
      $a_0 := 0$
      $b_0 := \max{1, N}$
      $x_0 := \tfrac{1}{2}(a_0 + b_0)$
      $n := 0$
      while $\textrm{intPow}(x_n, k) - N \neq 0$ AND $b_n - a_n > \tau$:
          if $\textrm{intPow}(x_n, k) - N < 0$:
              $a_{n+1} := x_n$
              $b_{n+1} := b_n$
          else:
              $a_{n+1} := a_n$
              $b_{n+1} := x_n$
          $n \mapsto n+1$
          $x_n := \tfrac{1}{2}(a_n + b_n)$
      return $x_n$
\end{lstlisting}\\

The proof that this converges to the correct value is very similar to the proof for square roots.\\

We can now consider the accuracy that can be acheived by our algorithm, for our purposes we will be considering \(\sqrt{N}\), though the same applies for \(\sqrt{k}{N}\). We know that \(\sqrt{N} \in [a_n, b_n] \forall n \in \N\), and in particular we know that either \(\sqrt{N} \in [a_n, x_n]\) or \(\sqrt{N} \in [x_n, b_n] \forall n \in \N\); therefore we know that \(\epsilon_n := \left|x_n - \sqrt{N}\right| \le \tfrac{1}{2}(b_n - a_n) \forall n \in \N\). Then as we know that \(b_{n+1} - a_{n+1} = \tfrac{1}{2}(b_n - a_n)\), we know that \(\epsilon_n \le \tfrac{1}{2^n}(b_0 - a_0)\).\\

We can consider that \(\forall N \in \Rpz \exists (r,k) \in [0,1]\times\Z : N = r \cdot 10^{2k}\); using this we know that \(\sqrt{N} = \sqrt{r} \cdot 10^k\). As we have the fixed initial bounds of \(a_0 = 0\) and \(b_0 = 1\), then if we are finding \(\sqrt{r}\) we know that \(\epsilon_n \le \tfrac{1}{2^n} \forall n \in \N\). Hence we can calculate the precision of our current estimate beforehand for any \(n \in \N\), and thus we can guarantee \(d\) significant digits of accuracy for \(r \in [0, 1]\).\\

To get this accuracy must find \(n \in \N\) such that \(\epsilon_n \le \tfrac{1}{10^d}\), to acheive this we must find \(n \in \N\) such that \(2^n \ge 10^d\). For example the following table indicates the required \(n\), required for certain significant digits of accuracy.

\begin{center}
	\begin{tabular}{|p{3cm}|p{3cm}|}
	\hline
	\(d\) & \(n : 2^n \ge 10^n\)\\
	\hline
	1 & 0\\\hline
	5 & 15\\\hline
	10 & 30\\\hline
	20 & 64\\\hline
	50 & 163\\\hline
	100 & 329\\\hline
	\end{tabular}
\end{center}
\TODO{Finish up writeup of code implementation, testing and examination}

%SUB%
\subsection{Newton's Method for Square Roots}
\label{SUB_"Newton for Square Roots"}

\theoremstyle{plain}
\newtheorem{SRNM Right-hand Convergence}{Proposition}[subsection]
\newtheorem{SRNM NR1 and NR2}[SRNM Right-hand Convergence]{Proposition}
\newtheorem{SRNM NR3 for v3}[SRNM Right-hand Convergence]{Proposition}

If we consider $f(x) = x^2 - N$ then if $x^\ast$ is a solution to $f(x) = 0$ we see that $x^\ast = \sqrt{N}$. As $f'(x) = 2x$, then the Newton's Method, will give $x_{n+1} = x_n - \frac{x^2 - N}{2x}$, where $x_0$ is a given initial guess.\\

We can see that, in C, each iteration will calculate \codeinline{x = x - (x*x - N) / (2*x)}, which requires 5 operations; however if we re-arrange our equation, we instead get \(x_{n+1} = \frac{1}{2}\right(x_n + \frac{N}{x}\right)\). Implementing our new iterative formula we get \codeinline{x = 0.5 * (x + N/x)}, which now uses only 3 operations.\\

We can then use the following pseudocode as the basis of our implementaions of the Newton-Raphson Method for Square Roots:

%PCD%
\label{PCD_"Newton Square Root Basic"}
\begin{lstlisting}[frame=single,mathescape,caption={Basic Newton Method for Square Root}]
  NewtonSquareRoot($N \in \R, x_0 \in \R, \tau \in (0,1)$):
      $n := 0$
      loop:
          $x_{n+1} := \tfrac{1}{2}(x_n + \tfrac{N}{x_n})$
          $\delta_n := |x_{n+1} - x_n|$
          if $\delta_n \leq \tau$:
              return $x_{n+1}$
          $n \mapsto n + 1$
\end{lstlisting}

Next we want to consider our initial estimate \(x_0\); it is prudent to first consider when our initial estimate will converge to the correct root. By looking at a graph of the function, and in particular the tangents to the curve, it would seem reasonable to wonder if \(\lim_{n\to\infty} x_n = \sqrt{N}\).

%THM%
\begin{SRNM Right-hand Convergence}
\label{THM_"SRNM Right-Hand Convergence"}
If \(x_0 \in \right(\sqrt{N}, \infty\right)\) and \(\left\{x_n : n\in\N\right\}\) is a sequnence of approximations of \(\sqrt{N}\) found via the Newton-Raphson Method, as detailed above, then:
\[\lim_{n\to\infty} x_n = \sqrt{N}\]
\end{SRNM Right-hand Convergence}

%PRF%
\begin{proof}
Suppose \(x_n > \sqrt{N}\), then
\begin{align*}
	x_{n+1} &= \frac{1}{2}\left(x_n + \frac{N}{x_n}\right)\\
		  &< \frac{1}{2}\left(x_n + \frac{N}{\sqrt{N}}\right) 
		  		&\mathrm{as } \sqrt{N} < x_n \implies \frac{1}{x_n} <
				\frac{1}{\sqrt{N}}\\
		  &= \frac{1}{2}\left(x_n + \sqrt{N}\right)\\
		  &< \frac{1}{2}(2x_n)\\
		  &= x_n
\end{align*}
Therefore we see that \(\left\{x_k : k \in [n, \infty) \cap \Z\right\}\) is a strictly decreasing sequence.\\
Now suppose that \(x_n \ge \sqrt{N}\) and then, for a contradiction, assume that \(x_{n+1} < \sqrt{N}\). We then see that:
\begin{align*}
	& \frac{1}{2}\left(x_n + \frac{N}{x_n}\right) < \sqrt{N}\\
	\implies & x_n + \frac{N}{x_n} < 2\sqrt{N}\\
	\implies & x_n^2 + N < 2\sqrt{N}x_n\\
	\implies & x_n^2 - 2\sqrt{N}x_n + N < 0\\
	\implies & \left(x_n - \sqrt{N}\right)^2 < 0
\end{align*}
This is a contradiction as \(x_n, \sqrt{n} \in \R \implies \left(x_n - \sqrt{N}\right)^2 \ge 0\).\\
Therefore \(x_n \ge \sqrt{N} \implies x_{n+1} \ge \sqrt{N}\).\\
Hence if \(x_0 > \sqrt{N}\), then it follows that \(\{x_n : n \in \N\}\) is a strictly decreasing sequence that is bounded below. Therefore by an elementary result from limit theory, we see that \(\lim_{n\to\inft} x_n = \sqrt{N}\).
\end{proof}\\

The most obvious choice for \(x_0\) would be \(N\), but we see that \(N \in (0,1)\), then \(N < \sqrt{N}\). In this case, we could choose \(x_0 = 1\) for the case that \(N \in (0,1)\). Therefore we can choose 
\[x_0 := \left\{\begin{array}{lcl}N &: &N \in\left(1,\infty\right)\\1 &: &N \in (0,1)\end{array}\right.\]\\

In our choice of \(x_0\), we have so far left out the cases where \(N \in \{0, 1}\). In both of these case we already know the correct answer, namely \(\sqrt{N} = N\) provided \(N \in {0, 1}\). Therefore we can exclude them from our calculations, as we can pre-asses the value of \(N\), simply returning the correct answer if one of these cases is encountered.\\

This then leads to an updated version of the above pseudocode:\\

%PCD%
\label{PCD_"Newton Square Root v1"}
\begin{lstlisting}[frame=single,mathescape,caption={Basic Newton Method for Square Root}]
  NewtonSquareRoot($N \in \Rpz, \tau \in (0,1)$):
      if $N \in \{0, 1\}$:
          return $N$
      if $N > 1$:
          $x_0 := N$
      else:
          $x_0 := 1$
      $n := 0$
      loop:
          $x_{n+1} := \tfrac{1}{2}(x_n + \tfrac{N}{x_n})$
          $\delta_n := |x_{n+1} - x_n|$
          if $\delta_n \leq \tau$:
              return $x_{n+1}$
          $n \mapsto n + 1$
\end{lstlisting}

\TODO{Write up examination and implementation of this pseudocode}\\

An alternative would be to use the integer square root method discussed in Section \ref{SUB_"Digit by Digit Method"} to improve our initial choice of \(x_0\). We will start by showing, that for intervals \(I \subset \Rp\), the first two criteria for quadratic convergence of the Newton Raphson method are met.

%THM%
\begin{SRNM NR1 and NR2}
\label{THM_"SRNM NR1 and NR2}
If \(I \subset \Rp\) then \(NR_1\) and \(NR_2\) are satisfied for \(f(x) = x^2 - N\)
\end{SRNM NR1 and NR2}

%PRF%
\begin{proof}
\(f(x) = x^2 - N \implies f'(x) = 2x \implies f''(x) = 2\)\\
Now as \(x \in \Rp \forall x \in I\), then it is obvious that \(f'(x) > 0\)\\
Therefore \(f'(x) \neq 0 \forall x \in I\), and so \(NR_1\) is satisfied.\\
As \(f''(x)\) is a constant function, then it is continuous on all of \(\R\).\\
Hence \(f''(x)\) is continuous \(\forall x \in I\) and so \(NR_2\) is satisfied.
\end{proof}

Now the integer square root function will always produce a root that is at most a distance of \(1\) from \(\sqrt{N}\); therefore we can consider \(I = [\sqrt{N} - 1, \sqrt{N} + 1]\). Now if \(N \le 1\), then \(I \seubset \Rp\) and so we cannot guarantee the satisfaction of \(NR_1\). Therefore we can proceed with our analysis of the case that \(N > 1\).\\

If \(N > 1\) we need to find when we can satisfy \(NR_3\). First, we remember that \(M := \sup{\left|\tfrac{f''(x)}{f'(x)}\right| : x \in I}\) and \(\epsilon_0 := \left|x_0 - \sqrt(N)\right|\). Then to satisfy \(NR_3\), we must have that \(M\epsilon_0 < 1\).\\

We can guarantee that \(\epsilon_0 \le 1\) because \(x_0 \in I\) from the integer square root algortihm; therefore it suffices to find the situation where \(M < 1\). As both \(f'\) and \(f''\) are continuous and non-zero on \(I\) it follows that \(M = \sup{x^{-1} : x \in I} = (\sqrt{N} - 1)^{-1}\). We then see that:
\begin{displaymath}
	\begin{align*}
		M < 1 &\iff \sqrt{N} - 1 > 1\\
			  &\iff \sqrt{N} > 2\\
			  &\iff N > 4
	\end{align*}
\end{displaymath}

Therefore we can get the following new choice for \(x_0\), and thus new pseudocode:
\begin{displaymath}
	x_0 := \left\{\begin{array}{lcl}
		1 &: &N \in (0,1)\\
		N &: &N \in (1,4]\\
		intSqrt(N) &: &N \in (4, \infty)
	\end{array}\right.
\end{displaymath}

%PCD%
\label{PCD_"Newton Square Root v2"}
\begin{lstlisting}[frame=single,mathescape,caption={Basic Newton Method for Square Root}]
  NewtonSquareRoot($N \in \Rpz, \tau \in (0,1)$):
      if $N \in \{0, 1\}$:
          return $N$
      if $N < 1$:
          $x_0 := 1$
      else:
          if $N \le 4$:
              $x_0 := N$
          else:
              $x_0 := $ IntSqrt($N$)
      $n := 0$
      loop:
          $x_{n+1} := \tfrac{1}{2}(x_n + \tfrac{N}{x_n})$
          $\delta_n := |x_{n+1} - x_n|$
          if $\delta_n \leq \tau$:
              return $x_{n+1}$
          $n \mapsto n + 1$
\end{lstlisting}

\TODO{Write up examination of different versions tried, such as using \(x_0 = N\), etc...}\\

If we consider any \(N \in \Rpz\), then \(\exists a \in \left[\frac{1}{2}, 1\right), b \in \Z : N = a \times 2^b\). Finding this value would be a hard as finding the logarithm of \(N\) base 2, but due to the representation of numbers within C, both standard C and MPFR have functions that allow us to extract these two values with minimal computational expenditure.\\

This helps as we can then narrow our problem, to only finding \(\sqrt{a} : a \in \left[\frac{1}{2}, 1\right)\), and then calculating 
\begin{displaymath}
	\sqrt{N} = \sqrt{a} \times 2^{\left\lfloor \frac{b}{2} \right\rfloor} \times \alpha \ \mathrm{where}\  
	\alpha = \left\{
		\begin{array}{lcl}
			1 & : & b \in 2\Z \\
			\sqrt{2} & : & b \in \Zp\setminus2\Z \\
			\frac{1}{sqrt{2}} & : & b \in \Zn\setminus\2\Z
		\end{array}\right.
\end{displaymath}

We then get the following algorithm, which implements this:

%PCD%
\begin{lstlisting}[frame=single,mathescape,caption={Newton Method for Square Root v3},label={PCD_"Newton Method for Square Root v3"}]
  NewtonSquareRoot($N \in \Rpz, \tau \in (0,1)$):
      Let $(a, b) :\in \left[\tfrac{1}{2}, 1\right)\times\Z$ s.t. $N = a*2^b$
      $x_0 := 1$
      if $b \equiv 0 \textrm{mod}\ 2$:
          $\alpha := 1$
      else:
          if $b > 0$:
              $\alpha := \sqrt{2}$
          else:
              $\alpha := \tfrac{1}{\sqrt{2}}$
      $n := 0$
      loop:
          $x_{n+1} := \tfrac{1}{2}(x_n + \tfrac{a}{x_n})$
          $\delta_n := |x_{n+1} - x_n|$
          if $\delta_n \leq \tau$:
              return $\alpha\cdot x_{n+1} \cdot 2^{\left\lfloor\frac{b}{2}\right\rceil}$
          $n \mapsto n + 1$
\end{lstlisting}

We must first consider the fact that the algorithm requires the pre-calculation of both \(\sqrt{2}\) and \(\tfrac{1}{\sqrt{2}}\), to be able to calculate all values. However, it turns out we can use the algorithm itself to generate these values as \(2 = \tfrac{1}{2} \cdot 2^2\), and as the exponent of 2 is even then the algorithm does not require \(\sqrt{2}\) for this computation. Similarly \(\tfrac{1}{2} = \tfrac{1}{2} \cdot 2^0\), which again is an even exponent. We can thus run our algorithm to find an arbitrarily accurate values for \(\sqrt{2}\) and \(\tfrac{1}{\sqrt{2}}\) to allow us to run the algorithm for other values.\\

With this observation can then consider \(N \in \left[\tfrac{1}{2}, 1\right)\). As this is a small range and, as per our previous algorithm, we use an initial guess of \(x_0 = 1\), then we can prove that our algorithm will converge quadratically to \(\sqrt{N}\).

%THM%
\begin{SRNM NR3 for v3}
\label{THM_"NR3 for v3"}
Algorithm \ref{PCD_"Newton Method for Square Root v3"}, satisfies the criteria of Theorem \ref{THM_"Quad Conv Newton"}, and thus has quadratic convergence to \(\sqrt{N}\).
\end{SRNM NR3 for v3}
\begin{proof}
To fulfill the criteria of Theorem \ref{THM_"Quad Conv Newton"}, we must find and interval \(I := [\sqrt{N}-r, \sqrt{N} + r]\) for some \(r \ge \epsilon_0\).\\

Consider \(\epsilon_0 = |\sqrt{N} - x_0| = 1 - \sqrt{N}\). We see that as \(N \ge \frac{1}{2}\) then \(\sqrt{N} \ge \sqrt{2}^-1\), and thus \(\epsilon_0 \le 1 - \sqrt{2}^-1\). Let us have \(r := 1 - \frac{1}{\sqrt{2}}\), and \(I\) as defined above.\\

If we look at the lower bound of \(I\), then we see that:
\begin{displaymath}
\begin{align*}
\sqrt{N} - r &\ge \frac{1}{\sqrt{2}} - (1 - \frac{1}{\sqrt{2}})\\
	&= \frac{2}{\sqrt{2}} - 1\\
	&= \sqrt{2} - 1 \\
	&> 0
\end{align*}
\end{displaymath}

Therefore we see that \(I \subset \Rp\), and so by Proposition \ref{THM_"SRNM NR1 and NR2} we get that \(\mathrm{NR}_1\) and \(\mathrm{NR}_2\) ar satisfied. It then remains to show that \(\mathrm{NR}_3\) is satisfied on \(I\).\\

Now by the definition in Theorem \ref{THM_"Quad Conv Newton"}, we have that \(M = \sup\left\{\frac{1}{2}\left|\frac{f''(x)}{f'(y)}\right| : x, y \in I\right\}\). We know that \(I\) is bounded, \(f''(x) = 2\) and \(f'(x) = 2x\) meaning that \(\frac{1}{2}\left|\frac{f''(x)}{f'(y)}\right| = \frac{1}{f'(x)}\) as \(x \in \Rp\).\\ 

Therefore our problem is reduced to finding \(\max\left\{\frac{1}{2x} : x \in I\right\}\), which is equivalent to finding \(\min\{x : x \in I\} = \sqrt{N} - r\). Therefore by passing this information back up the chain we get that \[M = \frac{1}{2(\sqrt{N} - r)}\]\\

Then we see that:
\begin{displaymath}
\begin{align*}
M\epsilon_0 &= \frac{1 - \sqrt{N}}{2(\sqrt{N} - r)}\\
	&\le \frac{1 - \frac{1}{\sqrt{2}}}{2(\sqrt{N} - r)} 
		& \textrm{as } \sqrt{N} \ge \frac{1}{\sqrt{2}}\\
	&\le \frac{1 - \frac{1}{\sqrt{2}}}{2(\frac{1}{\sqrt{2}}-r)}
		& \textrm{as } \sqrt{N} \ge \frac{1}{\sqrt{2}}\\
	&= \frac{1 - \frac{1}{\sqrt{2}}}{2(\frac{2}{\sqrt{2}} - 1)}\\
	&= \frac{1 - \frac{1}{\sqrt{2}}}{2\sqrt{2}(1-\frac{1}{\sqrt{2}})}\\
	&= \frac{1}{2\sqrt{2}}\\
	&< 1 & \textrm{as } 2\sqrt{2} > 1
\end{align*}
\end{displaymath}

As we have confirmed that \(M\epsilon_0 < 1\), then we have confirmed that \(\mathrm{NR}_3\) is satisfied on \(I\), and so the algorithm converges quadratically to the desired root.
\end{proof}

Using the previous proposition we can, similar to our previous methods, consider how many iterations would be needed to reach a required tolerance. To start we consider that, as mentioned in the proof or Theorem \ref{THM_"Quad Conv Newton"}, that \(\epsilon_n \le (M\epsilon_0)^{2^n - 1}\epsilon_0\).\\

We know that \(M\epsilon_0 \le \frac{1}{2\sqrt{2}}\) and that \(\epsilon_0 \le 1 - \frac{1}{\sqrt{2}}\), giving:
\[\epsilon_n \le \left(\frac{1}{2\sqrt{2}}\right)^{2^n - 1}\left(1 - \frac{1}{\sqrt{2}}\right)\]

Thus if we want to acheive a tolerance of \(\epsilon_n \le \tau\), then it suffices to find \(n \in \N_0\) such that:
\[\left(\frac{1}{2\sqrt{2}}\right)^{2^n - 1} \le \tau\]

Then,
\[(2^n - 1)\log\left(\frac{1}{2\sqrt{2}}\right) \le \log\left(\frac{\tau}{1 - \frac{1}{\sqrt{2}}}\right)\]

By noting that \(\log(\frac{1}{a}) = - \log(a)\), then we get
\[(1-2^n)\log(2\sqrt{2}) \le \log\left(\frac{\tau}{1 - \frac{1}{\sqrt{2}}}\right)\]

Once this is rearranged we get the following inequality:
\[2^n \ge \frac{\log\left(\frac{2(\sqrt{2} - 1)}{\tau}\right)}{\log(2\sqrt{2})}\]

By taking logarithms again and re-arranging we get that
\[n \ge \frac{\log\left(\frac{\log\left(\frac{2(\sqrt{2} - 1)}{\tau}\right)}{\log(2\sqrt{2})}\right)}{\log(2)} = \log_2\left(\log_{2\sqrt{2}}\left(2\frac{\sqrt{2} - 1}{\tau}\right)\right)\]

Now for an example, suppose we want to know how many iterations we need to perform to find \(\sqrt{N}\) to within 10 decimal places, i.e. \(\tau = 10^-10 = 0.0000000001\). We remember that \(\sqrt{N} \in [\frac{1}{2}, 1)\), and then we will apply transformations to this value afterwards, therefore this is equivalent to finding 10 significant digits of accuracy for our square root (ignoring any loss of accuracy that may arrise from multiplications afterwards).\\

Now in this case we want to find \(n \in \N\) such that \(n \ge log_2(log_{2\sqrt{2}}(2\cdot10^{10}(\sqrt{2}-1)))\). Using Wolfram Alpha to calculate this value we get that we need \(n \ge 4.457144...\) and so we can take \(n = 5\). This means that we could modify our algorithm and implementation to do 5 fixed iterations of Newton's Method to guarantee at least 10 decimal places of accuracy.\\

In terms of efficiency versus accuracy tradeoff modifying the problem thus would improve it's efficiency by removing, now unneccesary, calculation and comparrison of \(\delta_n\) at each stage. However this does need a fixed guaranteed accuracy, and therefore such a program would no longer be suitable if we needed to calculate a square root accurate to 15 decimal places.\\

Below is a table that lists the minimum \(n \in \N\) such that \(n\) satisfies our inequality, where our tolerance is \(10^k\) for some \(k \in \N\). This will give us the maximum number of iterations that must be performed for the required accuracy.

\begin{center}
\begin{tabular}{|p{3cm}|p{3cm}|}
\hline
\(k : \tau = 10^k\) & \(n\)\\\hline
5 & 4 \\\hline
10 & 5 \\\hline
100 & 8 \\\hline
1,000 & 12 \\\hline
1,000,000 & 22\\\hline
\end{tabular}
\end{center}

%SUB%
\subsection{Newton's Inverse Square Root Method}
\label{SUB_"Newton's Inverse Square Root Method"}

\theoremstyle{plain}
\newtheorem{Inv Sqrt Quad Conv}{Proposition}[subsection]

As discussed in Section \ref{#SEC#}, computers are more efficient at multiplication over division. We would therefore prefer to find a way of utilising Neton's Method without having to perform any costly division operations.\\

If we consider \(f(x) = N - \frac{1}{x^2}\) then if \(x^\ast\) is a solution to \(f(x) = 0\) we see that \(x^\ast = \frac{1}{\sqrt{N}}\). As \(f'(x) = \frac{2}{x^3}\), then the Newton's Method, will give \[x_{n+1} = x_n - \frac{N - \frac{1}{x_n^2}}{\frac{2}{x_n^3}} = x_n\left(\frac{3}{2} - \frac{N}{2}x_n^2\right)\] where \(x_0\) is a given initial guess. As can be seen this algorithm requires no division if we multiply by real constants rather than the division implied above.\\

We can then consider that, similar to Algorithm \ref{PCD_"Newton Method for Square Root v3"}, any \(N\) can be represented as \(a \cdot 2^b\) where \(a \in \left[\tfrac{1}{2}, 1\right)\). This will, again allow us to narrow our problem to a known range of values, by using the following transormations.
\begin{displaymath}
\begin{align*}
N = a \cdot 2^b &\implies \tfrac{1}{N} = \tfrac{1}{a} \cdot 2^{-b}\\
	&\implies \tfrac{1}{\sqrt{N}} = \tfrac{1}{a}\cdot2^{\lfloor\frac{-b}{2}\rceil} \cdot \alpha
		&\alpha := \left\{
			\begin{array}{lcl}
				1 & : & b \equiv 0 \mod 2\\
				\sqrt{2} & : & b \equiv 1 \mod 2, b \in \Zn\\
				\frac{1}{\sqrt{2}} & : & b \equiv 1 \mod 2, b \in \Zp\\
			\end{array}\right.\\
	& \implies \sqrt{N} = N \cdot \tfrac{1}{\sqrt{a}} \cdot 2^{\lfloor\frac{-b}{2}\rceil} \cdot \alpha
\end{align*}
\end{displaymath}

Therefore we only need to calculate inverse square roots for values of \(N\) in the range \([\tfrac{1}{2}, 1)\). Thus giving us the following algorithm:\\

%PCD%
\begin{lstlisting}[frame=single,mathescape,caption={Newton Inverse Square Root Method},label={PCD_"Newton Inverse Square Root"}]
  NewtonSquareRoot($N \in \Rpz, \tau \in (0,1)$):
      Let $(a, b) :\in \left[\tfrac{1}{2}, 1\right)\times\Z$ s.t. $N = a*2^b$
      $x_0 := 1$
      if $b \equiv 0 \textrm{mod}\ 2$:
          $\alpha := 1$
      else:
          if $b > 0$:
              $\alpha := \tfrac{1}{\sqrt{2}}$
          else:
              $\alpha := \sqrt{2}$
      $n := 0$
      loop:
          $x_{n+1} := x_n(\tfrac{3}{2} + \tfrac{a}{2}x_n^2)$
          $\delta_n := |x_{n+1} - x_n|$
          if $\delta_n \leq \tau$:
              return $N\cdot\alpha\cdot x_{n+1} \cdot 2^{\left\lfloor\frac{-b}{2}\right\rceil}$
          $n \mapsto n + 1$
\end{lstlisting}

With this method we can once again consider it's convergence properties, in particular does it satisfy the criteria for quadratic convergence in Theorem \ref{THM_"Quad Conv Newton"}.

%THM%
\begin{Inv Sqrt Quad Conv}
\label{THM_"Inv Sqrt Quad Conv"}
Algorithm \ref{PCD_"Newton Inverse Square Root"} satisfies the criteria of Theorem \ref{THM_"Quad Conv Newton"}, and thus has quadratic convergence to \(\sqrt{N}\).
\end{Inv Sqrt Quad Conv}
\begin{proof}
We know that we only need to consider \(N \in [\frac{1}{2}, 1)\), and therefore \(\sqrt{N}^{-1} \in (1, \sqrt{2}]\). Also \(x_0 = 1\) and so we see that 
\[\epsilon_0 = |x_0 - \sqrt{N}^{-1}| = \sqrt{N}^{-1} - x_0 \le \sqrt{2} - 1\]

Now let \(r := \epsilon_0 = \sqrt{N} - 1\) and \(I := [\sqrt{N}^{-1} - r, \sqrt{N}^{-1}]\). If we consider the lower bound of I we see that \(\sqrt{N}^{-1} - (\sqrt{N}^{-1} - 1) = 1\), and in particular \(0 \notin I\).\\

Next we know that \(f(x) = N - x^{-2}\), and therefore we get \(f'(x) = 2x^{-3}\), \(f''(x) = -6x^{-4}\). It is obvious that \(\nexists x \in \R : f'(x) = 0\), which means that \(f'(x) \neq 0 \forall x \in I\) and so \(\mathrm{NR}_1\) is satisfied. Also as \(f''\) is only discontinuous at \(x = 0\) and \(0 \notin I\), then \(f''(x)\) is continuous \(\forall x \in I\), meaning this satisfies \(\mathrm{NR}_2\).\\

Now \(M = \sup\left\{\tfrac{1}{2}\left|\frac{2x^3}{6y^4}\right| : x, y \in I\rifht\}\), we can simplify the function we are trying to minimise to get \(\tfrac{1}{6}\frac{x^3}{y^4}\). It is obvious that in order to maximise this function we should find the largest possibe \(x\) and smallest possible \(y\), as both are positive. Hence by taking \(x = \sqrt{N}^{-1} + r\) and \(y = 1\), then \(M = \frac{1}{6}(2\sqrt{N}^{-1} - 1)^3 \le \frac{1}{6}(2\sqrt{2} - 1)^3\).\\

Now we consider \(M\epsilon_0\):\\

\begin{displaymath}
\begin{align*}
	M\epsilon_0 &=\frac{1}{6}(2\sqrt{N}^{-1} - 1)^3(\sqrt{N} - 1)\\
		&\le \frac{1}{6}(2\sqrt{2} - 1)^3(\sqrt{2} - 1)\\
		&\approx 0.42199376\ldots\\
		&< 1
\end{align*}
\end{displaymath}

Therefore as \(M\epsilon_0 < 1\) we have satisfied \(\mathrm{NR}_3\), and as such we have quadratic convergence of our method to \(\sqrt{N}^{-1}\).
\end{proof}

We now have two methods that converge quadratically to  
 
%SUB%
\subsection{Methods for other roots}
\TODO{Fill this out later}

